{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2bZtErTgdRn",
        "outputId": "76e8891f-f6aa-40c4-fa29-7b2fd06dd63b"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8Xg7j80vgfG9"
      },
      "outputs": [],
      "source": [
        "# Storing access in environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7tOIbRJkgght"
      },
      "outputs": [],
      "source": [
        "# Initializing spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nG-M8vKbghle"
      },
      "outputs": [],
      "source": [
        "# Initialize a Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "     .master('local[*]')\\\n",
        "    .appName(\"Iniciando com Spark\")\\\n",
        "    .config('spark.ui.port', '4050')\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tNUDpJZgyTa",
        "outputId": "acbcfea2-f36e-40a5-825a-9c9cb4a03cec"
      },
      "outputs": [],
      "source": [
        "# Download the ngrok files to use in the spark session\n",
        "!wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wQwUhY3jgzc8"
      },
      "outputs": [],
      "source": [
        "# Expose ngrok access server\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4A9zdWAoBBc",
        "outputId": "65ef5220-931f-45ba-f03c-6fed8e4ddb8c"
      },
      "outputs": [],
      "source": [
        "# Create a database\n",
        "spark.sql('CREATE DATABASE IF NOT EXISTS  desp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYtrAM8qIxCW"
      },
      "outputs": [],
      "source": [
        "# Command to position yourself within the desp database\n",
        "spark.sql('USE desp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuWPY6UqI0R4",
        "outputId": "f97d51d6-5925-47da-a991-65c545236b53"
      },
      "outputs": [],
      "source": [
        "# Insert the file into a dataframe and display it\n",
        "churn_df = spark.read.csv('/content/Churn.csv', sep=';', header=True, inferSchema=True)\n",
        "churn_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aYg4431XJ89F"
      },
      "outputs": [],
      "source": [
        "# Partition the churn_df by the geography column and then save it in the database as a table\n",
        "\n",
        "# After completing the command, you can see that within the spark-warehouse/churn directory, there is a folder based on each of the cardinal values\n",
        "# generated a folder, in which each of the folders contains the values ​​divided by the geography column (France, Germany, Spain)\n",
        "churn_df.write.partitionBy('Geography').saveAsTable('churn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFDguIonKes5",
        "outputId": "6b9ae3aa-cbdc-4219-8697-6617072b2954"
      },
      "outputs": [],
      "source": [
        "# During the table consultation, it can be seen that the data remained unchanged, with the following changes:\n",
        "# They are in ascending order and the column was positioned last\n",
        "spark.sql('select * from churn').show(100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis considerations\n",
        "\n",
        " In the files tab, within the collab, you will see that a spark-warehouse folder was created. Inside it there will be a directory called churn, where the partitioned files will be located in this directory.\n",
        "\n",
        "### Observation\n",
        "\n",
        " In Spark, files by default are saved in the parquet model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f31034b11340f484efceab0eb6d94becdf261a95b0140f3d34fc1900eaee3d71"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
